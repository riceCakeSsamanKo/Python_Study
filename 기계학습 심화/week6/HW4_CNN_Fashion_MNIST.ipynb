{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T06:49:46.521328Z",
     "start_time": "2024-04-16T06:49:46.382768Z"
    },
    "id": "X5cGnvi7A3Un"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T06:50:06.834037Z",
     "start_time": "2024-04-16T06:50:01.977882Z"
    },
    "id": "0Y2OsX2EQU3v"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# This mounts your google drive storage to this code\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m     10\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# This mounts your google drive storage to this code\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AryDcrN7QU31"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "root = '/content/gdrive/My Drive/Test/'\n",
    "\n",
    "# root = './'\n",
    "# if not os.path.exists(root):\n",
    "#     os.makedirs(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HVAXJ7DQU35"
   },
   "outputs": [],
   "source": [
    "# settings for data normalization\n",
    "# trans = transforms.Compose([transform.ToTensor(), transform.Normalize((0.5,),(1.0,))])\n",
    "trans = transforms.ToTensor()\n",
    "# trans = transforms.Compose([transforms.ToTensor(), transforms.RandomHorizontalFlip()])\n",
    "\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=trans,\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=trans,\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdDmKu9z_eu3"
   },
   "outputs": [],
   "source": [
    "# Linear Classification\n",
    "class LinearClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(28*28, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        return logits\n",
    "\n",
    "model = LinearClassification().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhPn_4CvsfV9"
   },
   "outputs": [],
   "source": [
    "# # Multi-layer Perceptron\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(28*28, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "\n",
    "# model = NeuralNetwork().to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4kyTJnAQU39"
   },
   "outputs": [],
   "source": [
    "# # define the CNN:\n",
    "# # two convolution layers followed by two linear layers\n",
    "\n",
    "# class MyConvNet(nn.Module):   # nn.Module should be inherited for the neural network configuration so that it can work with backpropagation APIs\n",
    "#     def __init__(self):    # define layers: two conv layers and two linear (fully connected) layers\n",
    "#         super(MyConvNet, self).__init__()\n",
    "#         self.conv_layers = nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, 5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2,2),\n",
    "#             nn.Conv2d(32, 64, 5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2,2),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(4*4*64, 512),\n",
    "#             nn.Linear(512, 10)\n",
    "#         )\n",
    "\n",
    "#     # network connection: two conv layers, each followed by relu and max-pooling with (2x2) kernel, and two linear layer\n",
    "#     def forward(self, x):\n",
    "#         logits = self.conv_layers(x)\n",
    "#         return(logits)\n",
    "\n",
    "\n",
    "# # generate neural net model\n",
    "# model = MyConvNet().to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZU3HDu0QU4B"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0uJzp41QU4G"
   },
   "outputs": [],
   "source": [
    "# training and test\n",
    "\n",
    "# epoch: 10\n",
    "# data dimension: batch_size x Channels x Height x Width (NCHW)\n",
    "for epoch in range(10):\n",
    "    # training phase\n",
    "    # model.train()\n",
    "    current_loss= 0.0\n",
    "    for batch_num, (x, target) in enumerate(train_loader):\n",
    "        x, target = x.to(device), target.to(device)\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out,target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_loss += loss\n",
    "\n",
    "        # display the training loss in every 100 batches\n",
    "        if (batch_num+1)%100 == 0 or (batch_num+1)%100 == len(train_loader):\n",
    "            print('epoch: %d, batch_num: %d, current_loss: %.3f' %(epoch, batch_num+1, current_loss/100))\n",
    "            current_loss = 0.0\n",
    "\n",
    "\n",
    "    # test phase\n",
    "    with torch.no_grad():\n",
    "        # model.eval()\n",
    "        total_samples = 0.0\n",
    "        correct_samples = 0.0\n",
    "        for (x, target) in test_loader:\n",
    "            x, target = x.to(device), target.to(device)\n",
    "            out = model(x)\n",
    "            pred = torch.argmax(out,1)\n",
    "            correct_samples += (pred == target).sum()\n",
    "        print('Accuracy: %.3f' %(100*float(correct_samples) / float(len(test_data))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdAbm_TvQU4N"
   },
   "outputs": [],
   "source": [
    "# Display Some test results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "# take one batch of the data\n",
    "test_data, test_labels = next(iter(test_loader))\n",
    "\n",
    "# if cuda_available:\n",
    "test_data, test_labels = test_data.to(device), test_labels.to(device)\n",
    "out = model(test_data)              # put the test data to the trained network\n",
    "pred = torch.argmax(out,1)          # prediction to the highest probability\n",
    "\n",
    "\n",
    "sample_index = np.random.choice(batch_size, size=12)     # take 12 random sample index\n",
    "num_samples = sample_index.size\n",
    "\n",
    "random_samples = test_data[sample_index].cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# display test samples\n",
    "for k in range(num_samples):\n",
    "    plt.subplot(4, 4, k + 1)\n",
    "    plt.imshow(random_samples[k].reshape(28, 28),cmap='Greys')\n",
    "    plt.title(\"True: \" + str( classes[test_labels[sample_index[k]].item()]) + \",\\nPred: \" + str(classes[pred[sample_index[k]].item()]))\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhPBxl3OO09E"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model, root + 'my_CNN_model.pth')\n",
    "\n",
    "# load model\n",
    "# model = torch.load('my_CNN_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLBE-ZKbP0oJ"
   },
   "outputs": [],
   "source": [
    "# # save model weights\n",
    "# torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# # load model weights\n",
    "# model = MyConvNet()\n",
    "# model.load_state_dict(torch.load('model_weights.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
